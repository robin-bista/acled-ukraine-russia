{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp 2022-10-01-2022-11-10.csv s3://robin/acled/raw/2022-10-01-2022-11-1.csv\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "\n",
    "df = pd.read_csv(\"2022-10-01-2022-11-10.csv\", sep=\";\")\n",
    "df['event_date'] = pd.to_datetime(df['event_date'])\n",
    "\n",
    "def get_secret(secret_name):\n",
    "    region_name = \"us-east-1\"\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name)\n",
    "    get_secret_value_response = client.get_secret_value(SecretId=secret_name)\n",
    "    get_secret_value_response = json.loads(get_secret_value_response['SecretString'])\n",
    "    return get_secret_value_response\n",
    "\n",
    "db_credentials = get_secret(secret_name='robin')\n",
    "\n",
    "USERNAME = db_credentials[\"RDS_POSTGRES_USERNAME\"]\n",
    "PASSWORD = db_credentials[\"RDS_POSTGRES_PASSWORD\"]\n",
    "HOST = db_credentials[\"RDS_POSTGRES_HOST\"]\n",
    "PORT = 5432\n",
    "DBNAME = \"postgres\"\n",
    "CONN = f\"postgresql://{USERNAME}:{PASSWORD}@{HOST}:{PORT}/{DBNAME}\"\n",
    "\n",
    "# load the data into our postgres database\n",
    "alchemyEngine = create_engine(CONN, pool_recycle=3600);\n",
    "postgreSQLConnection = alchemyEngine.connect();\n",
    "\n",
    "df.to_sql('acled', postgreSQLConnection, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from time import strftime\n",
    "import requests\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.hooks.base_hook import BaseHook\n",
    "from airflow.operators.empty import EmptyOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.amazon.aws.operators.glue_crawler import GlueCrawlerOperator\n",
    "from airflow.models import Variable\n",
    "\n",
    "aws_conn = BaseHook.get_connection('aws_default')\n",
    "aws_access_key = aws_conn.login\n",
    "aws_secret_key = aws_conn.password\n",
    "acled_bucket = Variable.get(\"acled_bucket\")\n",
    "acled_crawler_name = Variable.get(\"acled_crawler_name\")\n",
    "owner = \"robin\"\n",
    "\n",
    "def ingest_data(date):\n",
    "    \"\"\"Ingest ACLED data for a given date\"\"\"\n",
    "\n",
    "    conf = SparkConf()\n",
    "    conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4')\n",
    "    conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "    conf.set('spark.hadoop.fs.s3a.access.key', aws_access_key)\n",
    "    conf.set('spark.hadoop.fs.s3a.secret.key', aws_secret_key)\n",
    "    spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "    \n",
    "    conn = BaseHook.get_connection('postgres')\n",
    "    engine = create_engine(f'postgresql://{conn.login}:{conn.password}@{conn.host}:{conn.port}/{conn.schema}')\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM acled where event_date > '{date}'::date - '10 days'::interval and event_date <= '{date}'::date\", engine)\n",
    "    dataframe = spark.createDataFrame(df)\n",
    "    \n",
    "    if dataframe.count() == 0:\n",
    "        return \"No data\"\n",
    "\n",
    "    dataframe = dataframe.withColumn(\"event_date\", F.to_date(\"event_date\", \"yyyy-MM-dd\"))\n",
    "    dataframe = dataframe.withColumn(\"fatalities\", dataframe[\"fatalities\"].cast(\"int\"))\n",
    "    dataframe = dataframe.withColumn(\"geo_precision\", dataframe[\"geo_precision\"].cast(\"int\"))\n",
    "    dataframe = dataframe.withColumn(\"inter1\", dataframe[\"inter1\"].cast(\"int\"))\n",
    "    dataframe = dataframe.withColumn(\"interaction\", dataframe[\"interaction\"].cast(\"int\"))\n",
    "    dataframe = dataframe.withColumn(\"latitude\", dataframe[\"latitude\"].cast(\"double\"))\n",
    "    dataframe = dataframe.withColumn(\"longitude\", dataframe[\"longitude\"].cast(\"double\"))\n",
    "    dataframe = dataframe.withColumn(\"time_precision\", dataframe[\"time_precision\"].cast(\"int\"))\n",
    "    dataframe = dataframe.withColumnRenamed(\"timestamp\", \"upload_date\")\n",
    "    dataframe = dataframe.withColumn(\"upload_date\", F.from_unixtime(\"upload_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    dataframe = dataframe.withColumn(\"upload_date\", F.to_timestamp(\"upload_date\", \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "    for column in dataframe.columns:\n",
    "        dataframe = dataframe.withColumn(column, F.when(F.col(column) == \"\", None).otherwise(F.col(column)))\n",
    "\n",
    "    dataframe.coalesce(1).write.partitionBy('event_date').mode(\"append\").option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\").parquet(f's3a://{acled_bucket}/acled_{owner}/')\n",
    "    return \"Success\"\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"RRB_ACLED_ETL_GLUE\",\n",
    "    start_date=datetime(2022, 10, 22),\n",
    "    schedule_interval=\"@weekly\",\n",
    "    catchup=False,\n",
    "    default_args={\n",
    "        \"retries\": 0,\n",
    "        \"email_on_failure\": False,\n",
    "        \"email_on_retry\": False,\n",
    "        \"owner\": owner\n",
    "    },\n",
    ") as dag:\n",
    "\n",
    "    start_task = EmptyOperator(task_id=\"acled_start_task\", dag=dag)\n",
    "\n",
    "    ingest_task = PythonOperator(\n",
    "        task_id=\"acled_ingest_task\",\n",
    "        python_callable=ingest_data,\n",
    "        op_args={\"{{ds}}\"},\n",
    "        dag=dag,\n",
    "    )\n",
    "    \n",
    "    glue_crawler_config = {\n",
    "        'Name': f'{acled_crawler_name}_{owner}',\n",
    "        'Role': 'arn:aws:iam::684199068947:role/service-role/AWSGlueServiceRole-FullS3Access',\n",
    "        'DatabaseName': 'datalake',\n",
    "        'Targets': {'S3Targets': [{'Path': f'{acled_bucket}/acled_{owner}'}]},\n",
    "    }\n",
    "\n",
    "    crawler_task = GlueCrawlerOperator(\n",
    "        task_id = \"acled_crawler_task\",\n",
    "        aws_conn_id='aws_default',\n",
    "        config = glue_crawler_config,\n",
    "        wait_for_completion=True,\n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "    end_task = EmptyOperator(task_id=\"acled_end_task\", dag=dag)\n",
    "\n",
    "    start_task >> ingest_task >> crawler_task >> end_task"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08d5e7c46d3fc40c26ff86f40ece739fe464b93577d91e92712d548f7feefc06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rwyn-env': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
